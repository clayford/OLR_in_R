---
title: "Ordinal Logistic Regression in R"
author: "Clay Ford, UVA Library"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
# trees is a data set that comes with R
pairs(trees)
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## CODE ALONG 0

Try the `cumsum()` function with the vector `1:5`

```{r}
cumsum(1:5)
```

## Load Packages

We'll use the following packages in today's workshop.

```{r}
library(vcd)
library(MASS)
library(car)
library(effects)
library(ggplot2)
library(ggeffects)
```

## Review: Probabilities and Cumulative Probabilities

Let's say I randomly sample 100 UVA students and ask them to rate how spicy they like their food on a scale of 1 - 5, where 1 is no spice and 5 is maximum spiciness. Below are made up responses entered manually using a named vector.

```{r}
resp <- c("1 none" = 17,
          "2 mild" = 23,
          "3 medium" = 31,
          "4 hot" = 21,
          "5 maximum" = 8)
resp
```

I can calculate proportions from this data and use these as estimates of _probabilities_. Using the `proportions()` function on a vector of numbers returns the proportion of each value relative to the total of the vector.

```{r}
proportions(resp)
```

Based on this random sample I might estimate that a randomly selected UVA student has a probability of about 0.31 of liking medium spicy food. I'm using my observed proportion as an estimate of probability.

Another way to summarize the results is with _cumulative probabilities_. For example, what's the the probability a randomly selected student can handle _medium heat or less_? Using the `cumsum()` we can calculate cumulative sums of proportions and answer this question. Below we take the proportions and "pipe" into the `cumsum()` function using the base R pipe operator. 

```{r}
proportions(resp) |> cumsum()
```

Based on this calculation I might estimate that a randomly selected student has a probability of about 0.71 of liking _medium or lower_ spiciness. We might state this mathematically as follows (using LaTeX):

$$P[X \le \text{medium}] = 0.71 $$

In this case we're using the _ordering of the responses_ to make additional inferences about student spiciness preference. 

What if we also collected additional information such as the sex of the respondents? Does being male or female affect the _cumulative probability_ of 0.71? For example, do we expect that to be higher for males?

What about age or weight? Does being older and heavier affect the _cumulative probability_ of spiciness preference?

_Ordinal Logistic Regression_ is a method that allows us to investigate questions such as this. It allows us to _model the cumulative probability_ of an ordered category given multiple predictors.

Performing Ordinal Logistic Regression is like fitting any other statistical model. We have to...

1. propose and fit a model
2. assess if the model is good
3. use the model to make predictions and/or quantify relationships

## Load Data for the workshop

Today we'll work with data from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis (Koch & Edwards, 1988). The data is available in the vcd package (Meyer et al, 2022)

```{r}
data("Arthritis", package = "vcd")
```

If for some reason you can't install the vcd package, you can also download the data from GitHub:

```{r}
URL <- "https://github.com/clayford/OLR_in_R/raw/main/data/arthritis.rds"
Arthritis <- readRDS(file = url(URL))
```

### List of variables

```{r}
names(Arthritis)
```

- ID: patient ID
- Treatment: factor indicating treatment (Placebo, Treated).
- Sex: factor indicating sex (Female, Male).
- Age: age of patient in years
- Improved: _ordered factor_ indicating treatment outcome (None, Some, Marked).

The goal today is to teach the basics of Ordinal Logistic Regression by developing a model to predict the _cumulative probability_ of pain improvement after treatment adjusting for age and sex. 


## Ordered Factors in R

An ordered factor is a _categorical variable with ordered levels_. Examples of ordered categorical variables include:

- Satisfaction/Recommendation ratings (Poor, Fair, Good, Excellent)
- Pain scales (none, mild, moderate, severe)
- Experience (none, some, proficient, expert)
- Likert scale ratings (disagree, neutral, agree; OR never, sometimes, always)

We can format a categorical variable as an ordered factor using the `factor()` function with the argument `ordered = TRUE`. _Note we are responsible for creating the order!_ Otherwise R will set the order alphabetically. 

Quick example using fake data. Sample from a vector 50 times with replacement using the `sample()` function. The result is a character vector.

```{r}
experience <- sample(c("none", "some", "proficient", "expert"), 
                     size = 50, replace = TRUE)
head(experience)
```

Let's convert to factor. Notice there is no ordering of the levels. They are simply listed alphabetically.

```{r}
experience <- factor(experience)
head(experience)
```

Now convert to _ordered factor_ using the argument `ordered = TRUE`. Notice the levels are ordered but still in alphabetical order. The notation `expert < none < proficient < some` says "expert" is less than "none", which is less than "proficient", which is less than "some". According to this ordering "some" is the highest, which doesn't make sense.

```{r}
experience <- factor(experience, ordered = TRUE)
head(experience)
```

Now let's do it the right way: Convert to ordered factor with the ordering of the levels specified using the `levels` argument. 

```{r}
experience <- factor(experience, ordered = TRUE, 
                     levels = c("none", "some", "proficient", "expert"))
head(experience)
```

This is an important data wrangling step when preparing for Ordinal Logistic Regression.

## Explore the Arthritis data

Our response is the Improved variable. Notice it's already an ordered categorical variable:

```{r}
head(Arthritis$Improved)
```

Let's tabulate the counts using `xtabs()`. The notation `~ Improved` means "tabulate by Improved".

```{r}
xtabs(~ Improved, data = Arthritis)
```

We an get cumulative proportions using `proportions()` and `cumsum()`. 

```{r}
xtabs(~ Improved, data = Arthritis) |>
  proportions() |>
  cumsum()
```

About 67% of subjects experienced None or Some improvement. Therefore about 33% of subjects experienced marked improvement. 

How does cumulative probability of Improved breakdown between the Treatments? The notation `~ Treatment + Improved` means "tabulate by Treatment and Improved, with Treatment in the rows and Improved in the columns." The `margin = 1` argument in `proportions()` says calculate row-wise proportions. Then we "apply" the `cumsum()` function to the rows of the table. Finally we use `t()` to rotate or "transpose" the table.

```{r}
xtabs(~ Treatment + Improved, data = Arthritis) |>
  proportions(margin = 1) |>
  apply(MARGIN = 1, cumsum) |> 
  t()
```

About 84% of subjects experienced None or Some improvement in the Placebo group versus 49% in the Treatment group. That implies only 16% experienced Marked improvement in the Placebo group versus 51% in the Treatment group.

Let's now stratify by Sex. We have some sparse cells in the Male table.

```{r}
xtabs(~ Treatment + Improved + Sex, data = Arthritis) |>
  proportions(margin = c(1,3)) |> 
  apply(MARGIN = c(1,3), cumsum) |>
  apply(MARGIN = c(1,3), t)

```

Finally let's look at how Age relates to Improved using a _spine plot_. It appears older subjects are somewhat more likely to see some improvement, at least for ages 35 - 60. 

```{r}
plot(Improved ~ Age, data = Arthritis)
```

## Ordinal Logistic Regression basics

Ordinal logistic regression produces a _model_ that we can use to make cumulative probability predictions or quantify how predictors affect cumulative probabilities. Like binary logistic regression, the results are on the log odds scale.

Let's fit a simple model for the Arthritis data. Below we model Improved as a function of Treatment. In other words, _how does Treatment affect the cumulative probability of Improved_?

One way to do this is with the `polr()` function from the MASS package. "polr" stands for "proportional odds logistic regression". We'll explain proportional odds in a moment. As with `lm()` and `glm()`, we define our model using R's formula syntax. We also typically want to save the model result and inspect the results using `summary()`

```{r}
m1 <- polr(Improved ~ Treatment, data = Arthritis)
summary(m1)
```

The message "Re-fitting to get Hessian" means the model was refit when we called `summary()` to get a matrix called the "Hessian" in order to compute standard errors. To suppress this message, use `Hess = TRUE` in the call to `polr()`.

Notice we have _multiple intercepts_ which are listed separately from the coefficients. The separate intercepts are for _the levels below which we calculate cumulative probabilities_. Recall the levels of Improved:

None < Some < Marked

There are two cumulative probabilities we can calculate:

1. None
2. Some and lower

(Marked and lower will always be 1.)

So we basically have two models:

1. log odds(None) = 0.7499 - 1.568*Treatment 
2. log odds(Some or lower) = 1.5499 - 1.568*Treatment

Where Treatment = 1 if treated, 0 if on Placebo.

Notice _the coefficient is subtracted from the intercept_! That's the model that `polr()` fits. Also notice the _model is on the log odds, or logit, scale_. Recall that log odds transforms probability from [0,1] to (-Inf, + Inf). So the model predicts log odds values, which we then transform back to probability using the inverse logit.

Now let's use our model and compute cumulative probabilities "by hand". Before we do that, it's good to know how to extract the intercepts and coefficients from the model object. The model object is a list. We use the `$` operator to extract elements from it. The intercepts are in the `zeta` element and the coefficients are in the `coefficients` element. (Can also use `coef()` to extract coefficients.)

```{r}
m1$zeta
```

```{r}
m1$coefficients
```

The log odds of None given a subject was on Placebo is simply the `None|Some` intercept: 0.7499107. To convert to probability, we can use the `plogis()` function. Below we pipe the `None|Some` intercept into `plogis()`

```{r}
(p1 <- m1$zeta["None|Some"] |> plogis())
```

The log odds of None given a subject was on Treatment is 0.7499107 - 1.567644. Again we convert to probability.

```{r}
(p2 <- (m1$zeta["None|Some"] - m1$coefficients) |> plogis())
```

So we predict about 68% of subjects on Placebo will see no improvement versus only about 31% of subjects on Treatment. 

We can do the same calculations for cumulative probability for Some improvement or lower: log odds(Some or lower) = 1.5499 - 1.568*Treatment

The log odds of Some or lower given a subject was on Placebo is simply the `Some|Marked` intercept: 1.5498756. 

```{r}
(p3 <- m1$zeta["Some|Marked"] |> plogis())
```

The log odds of Some or lower given a subject was on Treatment is 1.5498756 - 1.567644.

```{r}
(p4 <- (m1$zeta["Some|Marked"] - m1$coefficients) |> plogis())
```

This implies about 1 - 0.50 = 50% of Treated subjects can expect Marked improvement versus 1 - 0.82 = 18% of subjects on Placebo.

Let's compare the predicted cumulative probabilities to the observed cumulative proportions. 

```{r}
xtabs(~ Treatment + Improved, data = Arthritis) |>
  proportions(margin = 1) |>
  apply(MARGIN = 1, cumsum) |> 
  t()
```

Now compare to our model-based predicted cumulative probabilities. I'm laying out the previously calculated probabilities as a matrix to match the layout of `xtabs()`. 

```{r}
# model predictions
matrix(c(p1, p2, p3, p4, 1, 1), nrow = 2, 
       dimnames = list("Treatment" = c("Placebo", "Treated"),
                       "Improved" = c("None", "Marked", "Some")))
```

Our model-based predictions are quite close to the observed proportions, but not identical. This is because of the _proportional odds assumption_.


## Proportional Odds

Recall: odds = p/(1 - p)

Predicted probability of Improved = None on Treatment is 0.3062450. The odds are calculated as 0.3062450/(1 - 0.3062450)

```{r}
p2/(1 - p2)
```

Predicted probability of Improved = None on Placebo is 0.6791592. The odds are calculated as 0.6791592/(1 - 0.6791592)

```{r}
p1/(1 - p1)
```

The odds ratio for those two odds are

```{r}
(p2/(1 - p2))/
(p1/(1 - p1))
```

If we take the log of that we get the model coefficient for Treatment with a change in sign. (We explain why that is shortly.)

```{r}
# compare to coef(m1)
log((p2/(1 - p2))/
(p1/(1 - p1)))
```

Now let's do the same calculation for the Some or lower cumulative probabilities:

```{r}
# p4 = P(Some or lower) if Treated
# p3 = P(Some or lower) if Placebo
log((p4/(1 - p4))/
(p3/(1 - p3)))
```

Notice we get the _same answer_. The Treatment effect is proportional to the ordered category level used to determine cumulative probability. That's the proportional odds assumption. _The coefficients are independent of the levels of the ordered response_. It doesn't matter whether we go from "None" to "Some", or "Some" to "Marked", the effect of Treatment is the same.

Let's look at the coefficient from the model again.

```{r}
coef(m1)
```

This is identical to what we derived above, but with a change in sign. `polr()` multiplies the coefficients in the summary output by -1 so when you exponentiate and take the odds ratio, _higher levels of predictors correspond to the response falling in the higher end of the ordinal scale_.  

```{r}
exp(coef(m1))
```

Interpretation: the estimated odds that a Treated subject's response has higher improvement is about 4.8 times the odds that a Placebo subject's response has higher improvement

Notice if we "remove" the minus sign we get the odds ratio we calculated above.

```{r}
exp(-coef(m1))  # -coef(m1) is equivalent to -1*coef(m1)
```

Compare to:

```{r}
(p2/(1 - p2))/
(p1/(1 - p1))
```

And to:

```{r}
(p4/(1 - p4))/
(p3/(1 - p3))
```


## Summary output and confidence intervals

Let's look again at the output summary of our model. First we'll refit the model with Hess = TRUE to suppress the "re-fitting" message.

```{r}
m1 <- polr(formula = Improved ~ Treatment, data = Arthritis, 
           Hess = TRUE)
summary(m1)
```

The Standard Errors quantify the uncertainty in the estimated intercepts and coefficients. The t values are the ratios of values to standard errors. t values larger than 2 are good evidence that the sign on the coefficient is correct. In the output above, the t value of 3.53 for the coefficient is strong evidence that the effect of treatment is positive. 

To understand the uncertainty of the magnitude of the coefficient, we can calculate 95% confidence intervals using `confint()`:

```{r}
exp(confint(m1))
```

The estimated odds that a Treated subject's response has higher improvement is at least 2 times the odds that a Placebo subject's response has higher improvement, perhaps as high as 11 times.

## Using predict() with fitted models

Earlier we made predictions "by hand" for teaching purposes with intercept and coefficient values. In practice we would likely use `predict()` to get fitted probabilities. Using the `newdata` argument we can specify what we want to make predictions for. The default is to predict the class, or level, for the given predictors. (`type = "class"`)

The Placebo group is predicted to experience No improvement while the Treated groups is expected to experience Marked improvement.

```{r}
predict(m1, newdata = data.frame(Treatment = c("Placebo", "Treated")), 
        type = "class")
```

Changing `type = "probs` returns expected probabilities for each class.

```{r}
predict(m1, newdata = data.frame(Treatment = c("Placebo", "Treated")), 
        type = "probs")
```

Notice these are NOT cumulative probabilities. These are expected probabilities for each level. They sum to 1. We see that `type = "class"` setting is simply picking the category with the highest probability.

If we wanted cumulative probabilities, we need to use `cumsum()`

```{r}
predict(m1, newdata = data.frame(Treatment = c("Placebo", "Treated")), 
        type = "probs") |>
  apply(1,cumsum) |> 
  t()
```

## Visualizing the model

Between multiple intercepts, log odds, and cumulative probabilities, ordinal logistic regression output can be hard to interpret and communicate. Fortunately we have some methods for visualizing models. 

The effects package provides some good out-of-the-box lattice plots for visualizing ordinal logistic regression models. Use the `Effect` plot to specify which coefficient(s) you want to plot. In our model we only have one choice: Treatment. Then pipe into the `plot()` function. This results in a plot for each outcome. We can see the probability of Marked improvement is much higher for the Treated group than the Placebo group. Likewise the probability of no improvment is much lower in the Treated group than in the Placebo group.

```{r}
Effect(m1, focal.predictors = "Treatment") |> plot()
```

We can combine the lines into one plot as follows:

```{r}
Effect(m1, focal.predictors = "Treatment") |> 
  plot(lines=list(multiline=TRUE),
       confint=list(style="bars"))

```

Another visualization option is the "stacked" plot, though there is no indication of uncertainty. This plot is probably better suited for a numeric predictor.

```{r}
Effect(m1, focal.predictors = "Treatment") |> 
  plot(axes=list(y=list(style="stacked")))
```

If you want something created in ggplot2, the ggeffects package provides the `ggeffect()` function and associated plotting method. I think the `connect.lines = TRUE` argument is good to use.

```{r}
ggeffect(m1, terms = "Treatment") |> plot(connect.lines = TRUE)
```

## Model assessment

A good model should generate data that looks similar to our observed data. We can use the `simulate()` function to rapidly simulate many new sets of our response variable. Below we use our model to generate 250 new sets of the Improved variable. The result is a data frame with 250 columns, where each column is a simulated outcome from our model. The number of rows equals the sample size of our original data frame: 84. We're taking our 84 observed predictors and using our model to simulate the outcome, Improved.

```{r}
sims <- simulate(m1, nsim = 250)
dim(sims)
```

Here's a glimpse of the first 5 simulations:

```{r}
sims[1:5,1:5]
```


One quick assessment is to find the mean proportion of None, Some and Marked for the simulated data sets. Below we "apply" the function `proportions(xtabs(~x))` to each column of the "sims" data frame using `lapply()`. The result is a list object. We can combine the list elements into a matrix using `rbind()`. The `do.call()` function allows us to pass all those list elements to `rbind()`. Finally we take the mean of the three columns (None, Some and Marked) using `colMeans()`.

```{r}
tab_df <- lapply(sims, function(x)proportions(xtabs(~x))) |>
  do.call("rbind", args = _)
colMeans(tab_df)
```

Compare to observed proportions. Not bad!

```{r}
xtabs(~ Improved, data = Arthritis) |>
  proportions()
```

With a little more work we can plot the simulated proportions along with the observed proportions. First we need to save the observed proportions as a data frame.

```{r}
obs_p <- xtabs(~ Improved, data = Arthritis) |> 
  proportions() |>
  as.data.frame(responseName = "P")
obs_p
```

Now we need to "apply" this procedure to every column of our "sims" data frame. To do that we create a function. You'll notice the body of the function is basically the same code above, except `~ Improved, data = Arthritis` is replaced with `~ x`.

```{r}
p_df <- function(x){
  xtabs(~ x) |> 
  proportions() |>
  as.data.frame(responseName = "P")
  }
```

Now we apply the function to the "sims" data frame using `lapply` and then convert the result to a data frame by using `do.call` to feed every list element to the `rbind()` function.

```{r}
sims_df <- lapply(sims, p_df) |> 
  do.call(what = "rbind",args = _)
```

Now we're ready to create the plot. We can use multiple data frames in ggplot2 by calling the data frame directly in the `geom_` function. Again this looks good! Our simulated data is not systematically over-predicting or under-predicting.

```{r}
ggplot() +
  geom_point(mapping = aes(x = Improved, y = P), 
             data = obs_p, size = 3, color = "blue") +
  geom_jitter(mapping = aes(x = x, y = P), 
             data = sims_df, alpha = 0.25,
             width = 0.1, height = 0)
```

## Assessing the proportional odds assumption

Earlier we mentioned that proportional odds logistic regression assumes _the coefficients are independent of the category levels_. This means we assume the estimated odds ratio of Treatment, 4.795, is the same whether we're talking about comparing None versus Some or higher, or None or Some versus Marked on the Improvement scale.

One way to assess if this assumption is satisfied is the `poTest()` function in the car package. Simply call it on the fitted model. The null is the proportional odds assumption is true. Rejecting the null is evidence against the assumption. The `poTest()` function provides an overall test of the PO assumption as well as separate tests for each predictor. We only have one predictor so both tests are the same. The result below implies we're safe in our assumption of proportional odds.

```{r}
poTest(m1)
```

The authors of the car package state, "It's our experience that the proportional-odds assumption is rarely supported by a hypothesis test. The proportional-odds model can nevertheless provide useful simplification of the data as long as fitted probabilities under the model aren't substantially distorted." (Fox and Weisberg, 2019, p. 322)

I guess we just witnessed a rare event! Also, our simulation above showed the fitted probabilities were not "substantially distorted".

A graphical method for assessing the proportional odds assumption is available in the rms package. The function `plot.xmean.ordinaly` plots both...

- means of predictor by ordered category level (solid line)
- expected value of predictor given category level assuming PO assumption is true (dashed line)

When the solid and dashed lines roughly follow the same trajectory, we have good evidence that the PO assumption is safe. This looks great!

```{r}
# rms:: allows us to use the function without loading the rms package
rms::plot.xmean.ordinaly(Improved ~ Treatment, data = Arthritis)
```

According to Frank Harrell, Violation of Proportional Odds is Not Fatal. See https://www.fharrell.com/post/po/

If you think proportional odds assumption is badly violated, two options to consider: 

- The _partial proportional odds model_, which allows some predictors to have multiple coefficients that vary with the response level. This can be fit using the `vglm()` function in the VGAM package. 
- The _multinomial logit model_ that assumes no ordering of categories and fits a different coefficient for each response level. This can be fit using the `multinom()` function in the nnet package. 

## CODE ALONG

Now that we've laid out the basics of fitting, assessing and using an Ordinal Logistic Regression model, let's work through a more sophisticated example with additional predictors.

- Fit an ordinal regression model with Treatment, Sex, and Age. Call it `m2`. Include `Hess = TRUE` in `polr()`
- Assess proportional odds assumption
- Assess model fit using `simulate()`
- Interpret the coefficients
- Visualize the model

```{r}
m2 <- polr(Improved ~ Treatment + Sex + Age, 
           data = Arthritis, Hess = TRUE)
summary(m2)
```


```{r}
poTest(m2)
```

```{r}
rms::plot.xmean.ordinaly(Improved ~ Treatment + Sex + Age, data = Arthritis)
```

```{r}
sims2 <- simulate(m2, nsim = 250)
sims_df2 <- lapply(sims2, p_df) |> 
  do.call(what = "rbind",args = _)
ggplot() +
  geom_point(mapping = aes(x = Improved, y = P), 
             data = arth_p, size = 3, color = "blue") +
  geom_jitter(mapping = aes(x = x, y = P), 
             data = sims_df2, alpha = 0.25,
             width = 0.1, height = 0)
```

```{r}
exp(confint(m2))
```

Interpretation: 

- The estimated odds that a Treated subject's response has higher improvement is at least 2.3 times the odds that a Placebo subject's response has higher improvement, perhaps as high as 15 times, holding all other variables constant.
- The estimated odds that a Male subject's response has higher improvement is at least 20% lower (1 - 0.80) than the odds that a Female subject's response has higher improvement, perhaps as much as 91% lower, holding all other variables constant.
- The estimated odds that a subject has higher improvement increases by at least 0.3% for each 1 year increase in age, perhaps as much as 7%, holding all other variables constant.

Visualize the model with effect displays.

```{r}
ggeffect(m2, terms = "Age") |> plot()
```

```{r}
ggeffect(m2, terms = "Treatment") |> plot(connect.lines = TRUE)
```

```{r}
ggeffect(m2, terms = "Sex") |> plot(connect.lines = TRUE)
```

Is the model with three predictors an improvement over the model with 1 predictor?

```{r}
anova(m1, m2)
```

## We're done!

Thanks for coming! For free statistical consulting and training, contact us: `statlab@virginia.edu`


## References
- Chris Builder and Thomas Loughin (2015). Analysis of Categorical Data with R. CRC Press. URL: http://www.chrisbilder.com/categorical/
- John Fox and Sanford Weisberg (2019). An R Companion to Applied Regression, Third Edition. Thousand Oaks CA: Sage. URL: https://socialsciences.mcmaster.ca/jfox/Books/Companion/
- Michael Friendly and David Meyer (2016). Discrete Data Analysis with R. CRC Press. URL: http://ddar.datavis.ca/
- Frank Harrell (2015). Regression Modeling Strategies, 2nd edition. Springer.
- Lüdecke D (2018). “ggeffects: Tidy Data Frames of Marginal Effects from Regression Models.” _Journal of Open Source Software_, *3*(26), 772. doi:10.21105/joss.00772 <https://doi.org/10.21105/joss.00772>.
- David Meyer, Achim Zeileis, and Kurt Hornik (2022). vcd: Visualizing Categorical Data. R package version 1.4-10.
- G. Koch & S. Edwards (1988), Clinical efficiency trials with categorical data. In K. E. Peace (ed.), Biopharmaceutical Statistics for Drug Development, 403–451. Marcel Dekker, New York.
- Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0
- H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.